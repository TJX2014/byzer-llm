{
  "id" : "28",
  "name" : "电商场景",
  "user" : "william",
  "cell_list" : [ {
    "id" : "121",
    "content" : "--%markdown## 业务需求\n\n> 一个虚构的业务故事来描绘如何使用 Byzer-llm 解决实际业务问题\n\n假设我是一家电商企业，大概20-30人，目前在淘宝上已经有十万个SKU, 之前淘宝限制描述SKU的字数，所以写的特别精简。现在淘宝突然放开了，允许200字。那么这件事要不要做呢？肯定要做的，我们希望新的描述可以更好的吸引女性用户。这个时候人工去写这十万个描述么？在以前，可能真需要，因为AI很烂，而且性价比还低，还要招算法工程师。那现在呢？老板找了做微信小程序的研发，说两天之内把这事搞定。当然了，研发有两条路可以走，第一条是去调现成的大模型API。这个方式第一个是要花钱，第二个发送的数据很可能有被买卖的风险。 所以他选择了第二条路，找了一个开源的大模型，为了部署测试效果，他使用Byzer, 四条命令就把数据库里十万条记录都跑了一遍，发现效果不好。接着他自己花了第一天剩余的时间自己用excel标了1000条数据，然后上传后使用两条命令fintune了一把，重新部署成函数，再跑一把十万条记录，效果还不错。第二天，他把效果导出成excel给到老板看。老板满意，接着他用一条Byzer 指令把新的描述更新到了数据库，此时完成了整个业务场景。\n\n第三天，他刚准备泡杯茶休息下，老板来了一个新的需求，他希望把大概100万条数据里所有衣服的实体词都抽出来，比如高筒袜这样的。小哥用了三条Byzer 指令拿上次的大模型加上临时学习的一些prompt engineer的知识，快速的完成了这件事情。\n\n我们可以看到，对于业务而言，所谓大模型，只是一个更好用的工具。我们在现有的条件下，不依赖专业人员，直接去各种实际场景的解决业务问题，而不是利用AI聊天打屁。",
    "job_id" : null
  }, {
    "id" : "122",
    "content" : "--%markdown公司目前十万个SKU的标题都是这样的：\n\n\n```\n类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤\n```\n\n现在平台放开，可以展示更长的文本，需要将原本存量上述文字转化为更友好的描述,比如\n\n```\n宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，\n当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，\n还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。\n```\n\n我们使用大模型来完成这个工作。\n",
    "job_id" : null
  }, {
    "id" : "123",
    "content" : "-- 安装 Byzer 相关插件\n\n-- 可视化插件\n-- !plugin app add - \"byzer-yaml-visualization-3.3\";\n\n-- shell命令插件\n!plugin app add - \"mlsql-shell-3.3\";\n\n-- excel插件\n!plugin app add \"tech.mlsql.plugins.ds.MLSQLExcelApp\" \"mlsql-excel-3.3\";\n\n-- Byzer-LLM 插件\n!plugin app add - \"byzer-llm-3.3\";",
    "job_id" : "3deda745-292f-4a83-ab39-bcbd4d5ba726"
  }, {
    "id" : "125",
    "content" : "--%markdown## 加载和管理下载的模型\n\n我们通过 HuggingFace 或者网盘等下载站点下载大模型。在分布式环境里，为了避免模型需要反复的进行拷贝，\nByzer 推荐和数据一样，模型也是用数据湖存储，并且以表的形态存在。\n\n执行完下边语句后，就可以把模型保存成表了，并且具备版本化，可以在侧边栏看到。数据目录->Deltalake->ai_model中看到。\n",
    "job_id" : null
  }, {
    "id" : "124",
    "content" : "load model.`file:///home/winubuntu/projects/glm-model/chatglm-6b-model` \nas chatglm_6b_model;\n\nsave overwrite chatglm_6b_model as delta.`ai_model.chatglm_6b_model`;",
    "job_id" : "140b0974-6c49-4304-a26e-09f1a804241a"
  }, {
    "id" : "143",
    "content" : "-- 配置一些环境参数\n!python conf \"rayAddress=127.0.0.1:10001\";\n!python conf \"pythonExec=/home/winubuntu/miniconda3/envs/byzerllm-desktop/bin/python\";\n!python conf \"dataMode=model\";\n!python conf \"runIn=driver\";\n!python conf \"schema=st(field(value,string))\";\n\n!python conf \"num_gpus=1\";\n!python conf \"maxConcurrency=1\";\n!python conf \"standalone=true\";",
    "job_id" : "3b44a334-fc75-4db1-a081-abf9205a43f3"
  }, {
    "id" : "127",
    "content" : "--%markdown## 部署原始预训练大模型\n\n我们部署下模型，然后看看原始模型的效果。\n\n| Parameter | Description |\n|--|--|\n|`action=\"infer\"`| The action to perform - here running inference|\n|`pretrainedModelType=\"chatglm\"`| The model type - here ChatGLM. moss by default|\n|`localPathPrefix=\"/my8t/byzerllm/jobs\"`  | The local path you can specified to store the model in case /tmp is too small|\n|`modelWaitServerReadyTimeout=\"300\"`| The timeout in seconds when waiting for the model server|\n|`quantizationBit=\"true\"`|Whether to use quantization. This option is only works for ChatGLM. The Moss will auto detect according the model files|\n|`udfName=\"origin_model_predict\"`|The name of the user-defined predict function|\n|`modelTable=\"d_chatglm_6b_model\"`|The name of the model table|\n\n如果是单机部署，为了快速加载模型，也可以配置如下参数：\n\n```\nlocalModelDir=\"/home/winubuntu/projects/glm-model/chatglm-6b-model\"\n```\n\n指定 Ray 节点的本地地址，同时修改 modelTable=\"command\"\n",
    "job_id" : null
  }, {
    "id" : "126",
    "content" : "-- 从数据湖加载模型\nload delta.`ai_model.chatglm_6b_model` as d_chatglm_6b_model;\n\n-- 模型转化为预测函数，方便我们看原始模型的效果\nrun command as LLM.`` where \naction=\"infer\"\n-- chatglm/moss\nand pretrainedModelType=\"chatglm\"\n-- local path where we can storage the model in case the disk(tmp) is too small\nand localPathPrefix=\"/my8t/byzerllm/jobs\"\nand modelWaitServerReadyTimeout=\"300\"\nand quantizationBit=\"false\"\nand quantizationBitNum=\"4\"\nand udfName=\"origin_model_predict\"\nand modelTable=\"d_chatglm_6b_model\";",
    "job_id" : "de7c30db-3e97-4042-8cde-1ccebfc5c157"
  }, {
    "id" : "129",
    "content" : "--%markdown## 在批处理中验证效果\n\nByzer 会把模型部署成 SQL 函数。考虑到Byzer 支持批处理，流式计算，这意味着你可以快速的利用模型\n大规模处理一些数据或者在诸如风控中迅速使用大模型的能力。\n\n这里，我们用 SQL 随意写了一个问题，让大模型来回答。",
    "job_id" : null
  }, {
    "id" : "128",
    "content" : "-- 查看原始模型效果\nselect to_json(map(\n         \"instruction\",\"类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤\",\n        \"output\",\"NAN\")) as value as testData;\n\nselect origin_model_predict(array(value)) as r from testData as result; \n\nselect from_json(r[0],'array<struct<labels:string,predict:string>>')[0] as nr  from result as finalResult;\n\nselect nr.predict from finalResult as output;\n",
    "job_id" : "5d14b42e-0595-489b-ae0c-c43624af7ae6"
  }, {
    "id" : "132",
    "content" : "--%markdown## 对接业务库\n\nByzer 因为是面向 Data + AI 的 Infra, 所以 Byzer 无侵入式的对接市面主流数据源。\n这包括：\n\n1. 各种关系型数据库， MySQL, Oracle, DB2 等等。\n2. 各种对象存储的各种数据格式。诸如 阿里云，华为云，亚马逊，微软云上比如 Json, Csv, Parquet, Excel等各种格式。\n\n此外能够使用标准 SQL 对这些数据源的数据进行高效率混算。 ",
    "job_id" : null
  }, {
    "id" : "131",
    "content" : "-- 我们从已有的一个业务库加载数据，并且修改字段名称\n\nload jdbc.`business.tunningData` \nwhere url=\"jdbc:mysql://127.0.0.1:3306/business?characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&useSSL=false\"\nand driver=\"com.mysql.jdbc.Driver\"\nand user=\"root\"\nand password=\"${PASSWORD}\"\nas tunningData;\n\nselect content as instruction,\"\" as input,summary as output \nfrom tunningData as formatTunningData;",
    "job_id" : "e270261d-1d2b-42b2-b405-70d43d8a2539"
  }, {
    "id" : "144",
    "content" : "--%markdown## 对接手动标注的 Excel文件\n\n如果你是手动标注的数据，那么通过 Byzer Notebook 上传界面上传你的 Excel文件即可。\nByzer 可以直接加载 Excel 文件，示例如下：",
    "job_id" : null
  }, {
    "id" : "146",
    "content" : "load excel.`/tmp/upload/tunningData.xlsx` \nwhere header=\"true\" \nas tunningData;\n\nselect content as instruction,\"\" as input,summary as output \nfrom tunningData \nas formatTunningData;",
    "job_id" : "4094c9bd-046c-48e6-aae8-c8f767facf59"
  }, {
    "id" : "147",
    "content" : "--%markdown## 给大模型注入业务数据\n\n通过对前面数据的额处理，接着我们就可以直接把这些数据\n注入到大模型里面了。\n\n注入完成后会产生一个新的大模型，我们同样可以保存到数据湖按库表方式来管理。\n\n| Parameter | Description | \n|--|--|\n|`modelWaitServerReadyTimeout=\"300\"`| The timeout in seconds when waiting for the model server| \n|`dataWaitServerReadyTimeout=\"300\"`| The timeout in seconds when waiting for the data server|\n|`localPathPrefix=\"/my8t/byzerllm/tmp\"`| The local path to store temporary data e.g the original model, tunned model,tranining data|\n|`finetuneType=\"lora\"`| finetune way: lora/p_tuning/freeze|\n|`model=\"d_chatglm_6b_model\"`| The name of the model|\n|`inputTable=\"formatTunningData\"`| The name of the trianning data table|\n|`devices=\"0\"`| The GPU devices to use|\n|`maxSteps=\"3000\"`|The maximum number of steps|\n|`saveSteps=\"1000\"`|The number of steps per save checkpoint|\n|`outputTable=\"tunnignModel3000\"`|The name of the output table|\n\n\n如果是单机部署，为了快速加载模型，也可以配置如下参数：\n\n```\nlocalModelDir=\"/home/winubuntu/projects/glm-model/chatglm-6b-model\"\n```\n\n指定 Ray 节点的本地地址，同时修改 modelTable=\"command\"\n",
    "job_id" : null
  }, {
    "id" : "133",
    "content" : "load delta.`ai_model.chatglm_6b_model` as d_chatglm_6b_model;\n\n-- 测试模型微调\n!python conf \"schema=file\";\n!python conf \"runIn=executor\";\n\nrun command as LLM.`` where \n\nmodelWaitServerReadyTimeout=\"300\"\nand dataWaitServerReadyTimeout=\"300\"\nand localPathPrefix=\"/my8t/byzerllm/jobs\"\n\nand pretrainedModelType=\"chatglm\"\nand finetuneType=\"lora\"\n\nand model=\"d_chatglm_6b_model\"\nand inputTable=\"formatTunningData\"\n\nand devices=\"0\"\nand maxSteps=\"300\"\nand saveSteps=\"100\"\nand outputTable=\"tunnignModel300\";\n\n-- 保存模型到数据湖\n-- tunnignModel\nsave overwrite tunnignModel300 as delta.`ai_model.tunnignModel300`;",
    "job_id" : "d66352ae-3a57-41ba-893d-349953d0ea51"
  }, {
    "id" : "137",
    "content" : "--%markdown## 部署注入新知识的大模型\n\n我们现在可以把诸如了我们新数据的大模型部署成 SQL 函数方便在多场景中使用。\n使用方式和我们上面部署原始的模型是一致的。\n",
    "job_id" : null
  }, {
    "id" : "138",
    "content" : "-- 加载新模型\nload delta.`ai_model.tunnignModel3000` as tunnignModel;\n\n-- 一些环境配置\n!python conf \"runIn=driver\";\n\n-- 模型转化为预测函数\nrun command as LLM.`` where \naction=\"infer\"\nand pretrainedModelType=\"chatglm\"\nand localPathPrefix=\"/my8t/byzerllm/tmp\"\nand modelWaitServerReadyTimeout=\"300\"\nand quantizationBit=\"true\"\nand udfName=\"finetune_model_predict\"\nand modelTable=\"tunnignModel\";",
    "job_id" : "aa8ce1c7-e0a4-418c-bdfc-85d2525a49e1"
  }, {
    "id" : "139",
    "content" : "--%markdown## 对比效果\n\n我们可以同时用原始模型和新模型对同样的数据做预测，从而方便对比我们\n的业务数据是不是提升了模型的性能。",
    "job_id" : null
  }, {
    "id" : "140",
    "content" : "\n-- 通过SQL来验证效果（可以很方便的在批或者流式计算中使用我们的模型）\nselect '{\n        \"instruction\":\"类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤\",\n        \"output\":\"NAN\"\n    }' as value as testData;\n\nselect origin_model_predict(array(value)) as r1 ,finetune_model_predict(array(value)) as r from testData as result; \n\nselect from_json(r1[0],'array<struct<labels:string,predict:string>>')[0] as nr1 ,from_json(r[0],'array<struct<labels:string,predict:string>>')[0] as nr  from result as finalResult;\n\nselect nr1.predict as o,nr.predict as n  from finalResult as output;\n",
    "job_id" : "2123856e-a5f2-48ea-9278-d23a00d46978"
  }, {
    "id" : "148",
    "content" : "--%markdown\n## 做一丢丢 Prompt 工程让效果更好\n\n我们其实可以给定一些提示词，让模型表现更好。",
    "job_id" : null
  }, {
    "id" : "149",
    "content" : "set prompt=\"我给定你一段文本，你需要扩充这段文本，字数大概在100字，描述的内容尽量要吸引女性用户。下面是我给你的文本： \";\n\nselect '{\n        \"instruction\":\"${prompt} 类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤\",\n        \"output\":\"NAN\"\n    }' as value as testData;\n\nselect finetune_model_predict(array(value)) as r from testData as result; \n\nselect from_json(r[0],'array<struct<labels:string,predict:string>>')[0] as nr  from result as finalResult;\n\nselect nr.predict as n  from finalResult as output;",
    "job_id" : "f0125715-5dd8-4cfb-beee-ac194cdd8da9"
  }, {
    "id" : "141",
    "content" : "--%markdown## 通过 API 供外部应用使用\n\n现在我们可以通过 curl 来测试下我们的API。后续就可以在诸如各种应用中集成我们的新\n模型了。",
    "job_id" : null
  }, {
    "id" : "142",
    "content" : "\n-- 通过curl来验证我们也成功的将模型部署成了一个API服务\nset q = '''\"{\\\"instruction\\\":\\\"类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤\\\",\\\"output\\\":\\\"NAN\\\"}\"''';\n\n!sh curl -XPOST 'http://127.0.0.1:9003/model/predict' -d \n'sessionPerRequest=true&sessionPerUser=true&owner=william&dataType=string&sql=select finetune_model_predict(array(feature)) as a &data=[${q}]';",
    "job_id" : "b0e489f8-898d-46ad-8c96-d40b4f74641b"
  } ],
  "is_demo" : null
}